# **Neural Network from Scratch with Backpropagation**
ğŸš€ Implementation of a simple **Multi-Layer Perceptron (MLP)** from scratch using **NumPy**, with **Leaky ReLU activation** and **customizable layers**. This project demonstrates **forward propagation, backpropagation, and gradient descent**.

---

## **ğŸ“Œ Features**
âœ”ï¸ Fully connected **Neural Network (MLP)**  
âœ”ï¸ **Leaky ReLU activation** for hidden layers  
âœ”ï¸ **Gradient Descent Optimization**  
âœ”ï¸ **Customizable network architecture**  
âœ”ï¸ **Mathematical Backpropagation Derivation**  
âœ”ï¸ Implementation in **Python (NumPy)**  
âœ”ï¸ **Comparison between manual and code gradients**  
