# **Neural Network from Scratch with Backpropagation**
🚀 Implementation of a simple **Multi-Layer Perceptron (MLP)** from scratch using **NumPy**, with **Leaky ReLU activation** and **customizable layers**. This project demonstrates **forward propagation, backpropagation, and gradient descent**.

---

## **📌 Features**
✔️ Fully connected **Neural Network (MLP)**  
✔️ **Leaky ReLU activation** for hidden layers  
✔️ **Gradient Descent Optimization**  
✔️ **Customizable network architecture**  
✔️ **Mathematical Backpropagation Derivation**  
✔️ Implementation in **Python (NumPy)**  
✔️ **Comparison between manual and code gradients**  
