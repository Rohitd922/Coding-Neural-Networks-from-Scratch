{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlNDrM1sorDRDW9SEMpmJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohitd922/Coding-Neural-Networks-from-Scratch/blob/master/Neural_Network_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **This notebook implements a multi-layer perceptron (MLP) from scratch using numpy, with Leaky ReLU activation for hidden layers and a linear output layer.**"
      ],
      "metadata": {
        "id": "TzjKV099qi5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Constants Explanation**\n",
        "- `LEARNING_RATE = 0.1`: Controls how much the weights are adjusted during training.\n",
        "- `ALPHA = 0.01`: This is the slope for **Leaky ReLU** when  $z \\leq 0$.\n",
        "- `NUM_SAMPLES = 3`: We have **3 training examples**.\n"
      ],
      "metadata": {
        "id": "IHngjohSqxx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "LEARNING_RATE = 0.1  # Controls how much weights are adjusted during training\n",
        "ALPHA = 0.01  # Leaky ReLU coefficient (slope for negative values)\n",
        "NUM_SAMPLES = 3  # Number of training examples"
      ],
      "metadata": {
        "id": "ZRCLHb3AqcK9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Training Data Explanation**\n",
        "- `x_train.shape = (1,3)`:  \n",
        "  - **1 feature (input dimension)**.\n",
        "  - **3 samples (columns represent different training samples)**.\n",
        "- `y_train.shape = (1,3)`:  \n",
        "  - **1 output per sample**.\n",
        "\n",
        "### **Why Use This Shape?**\n",
        "- Ensures **correct matrix multiplication** with weight matrices during forward propagation.\n"
      ],
      "metadata": {
        "id": "RSoHtuAmrf78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data (features x samples)\n",
        "x_train = np.array([[2], [-1], [3]]).T  # Shape (1, 3) - 1 feature, 3 samples\n",
        "y_train = np.array([[3], [1], [5]]).T  # Shape (1, 3) - 1 output per sample\n"
      ],
      "metadata": {
        "id": "1vrWOSVtmcHD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Activation Functions**\n",
        "- **Leaky ReLU Function**:\n",
        "  - If $ z > 0 $ → return $ z $ (linear behavior).\n",
        "  - If $ z \\leq 0 $ → return $ ALPHA \\times z $ (small slope instead of 0).\n",
        "  \n",
        "  **This prevents vanishing gradients for negative values!**\n",
        "\n",
        "- **Leaky ReLU Derivative**:\n",
        "  - If $ z > 0 $ → derivative = **1**.\n",
        "  - If $ z \\leq 0 $ → derivative = **ALPHA** (small positive value).\n"
      ],
      "metadata": {
        "id": "iSLuuESvrvLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leaky ReLU Activation Function\n",
        "def leaky_relu(z):\n",
        "    \"\"\"Applies the Leaky ReLU activation function element-wise.\n",
        "\n",
        "    Args:\n",
        "        z: Input value or array.\n",
        "\n",
        "    Returns:\n",
        "        The result of applying Leaky ReLU to the input.\n",
        "    \"\"\"\n",
        "    return np.where(z > 0, z, ALPHA * z)  # If z > 0, return z; else, return ALPHA * z\n",
        "\n",
        "# Derivative of Leaky ReLU\n",
        "def leaky_relu_derivative(z):\n",
        "    \"\"\"Calculates the derivative of the Leaky ReLU function element-wise.\n",
        "\n",
        "    Args:\n",
        "        z: Input value or array.\n",
        "\n",
        "    Returns:\n",
        "        The derivative of Leaky ReLU at the input.\n",
        "    \"\"\"\n",
        "    return np.where(z > 0, 1, ALPHA)  # If z > 0, derivative is 1; else, it's ALPHA\n"
      ],
      "metadata": {
        "id": "oBfESGiErw8F"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Neural Network Class**\n",
        "- This `NeuralNetwork` class initializes and trains a **multi-layer perceptron (MLP)**.\n",
        "- The **layer sizes** are passed as a list.  \n",
        "  - Example: `[1, 4, 3, 1]` means:\n",
        "    - **1 input neuron**.\n",
        "    - **4 neurons in the first hidden layer**.\n",
        "    - **3 neurons in the second hidden layer**.\n",
        "    - **1 output neuron**.\n",
        "\n",
        "### **Weight & Bias Initialization**\n",
        "1. **Weight Initialization**:\n",
        "   - Each **weight matrix** has shape `(neurons in next layer, neurons in current layer)`.\n",
        "   - **Small random values** prevent exploding gradients.\n",
        "2. **Bias Initialization**:\n",
        "   - Each **bias vector** has shape `(neurons in next layer, 1)`.\n",
        "   - Biases start at **zero**.\n"
      ],
      "metadata": {
        "id": "_leu9eoOsL5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Forward Propagation**\n",
        "1. **Stores input features** as the first activation.\n",
        "2. **Loop through hidden layers**:\n",
        "   - Computes: $ Z = W \\cdot A + B $\n",
        "   - Applies **Leaky ReLU** activation for hidden layers.\n",
        "3. **Output Layer (No activation function)**:\n",
        "   - Linear activation (for regression).\n"
      ],
      "metadata": {
        "id": "Ndic1OLksi5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Compute Loss (Mean Squared Error)**\n",
        "- Uses **Mean Squared Error (MSE)**:\n",
        "  $$\n",
        "  Loss = \\frac{1}{2} (y - \\hat{y})^2\n",
        "  $$\n"
      ],
      "metadata": {
        "id": "vnvIWscGs4NF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Backpropagation – Detailed Explanation**\n",
        "Backpropagation is the process of computing the **gradients** of the loss function with respect to the weights and biases, and then **updating them using gradient descent**.\n",
        "\n",
        "### **Goal:**  \n",
        "- Adjust weights and biases to **minimize the loss function** by propagating errors backward through the network.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 1: Compute Gradients for Output Layer**\n",
        "For the output neuron, we use **Mean Squared Error (MSE)** as the loss function:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{2N} \\sum (y - a_2)^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ N $ is the number of samples.\n",
        "- $ y $ is the true label.\n",
        "- $ a_2 $ (or $ \\hat{y} ) $ is the predicted output.\n",
        "\n",
        "#### **Derivative of Loss with Respect to Output Activation**\n",
        "The gradient of loss w.r.t. the output activation $ a_2 $ is:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{da_2} = (a_2 - y)\n",
        "$$\n",
        "\n",
        "Since the **output layer activation is linear**, we also get:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{dz_2} = \\frac{dL}{da_2} = (a_2 - y)\n",
        "$$\n",
        "\n",
        "Now, we compute the gradients for the **output layer parameters** \\( w_2 \\) and \\( b_2 \\):\n",
        "\n",
        "$$\n",
        "\\frac{dL}{dw_2} = \\frac{1}{N} \\sum (a_2 - y) \\cdot a_1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dL}{db_2} = \\frac{1}{N} \\sum (a_2 - y)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 2: Compute Gradients for Hidden Layer**\n",
        "The hidden layer uses **Leaky ReLU activation**:\n",
        "\n",
        "$$\n",
        "a_1 = \\text{LeakyReLU}(z_1) = max(αz_1, z_1)\n",
        "$$\n",
        "\n",
        "The gradient of loss w.r.t. hidden layer activation is:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{da_1} = \\frac{dL}{dz_2} \\cdot w_2\n",
        "$$\n",
        "\n",
        "Applying the **Leaky ReLU derivative**:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{dz_1} = \\frac{dL}{da_1} \\cdot \\text{LeakyReLU}'(z_1)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\text{LeakyReLU}'(z_1)\n",
        "\\begin{cases}\n",
        "1, & \\text{if } z_1 > 0 \\\\\n",
        "\\alpha, & \\text{if } z_1 \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Now, we compute the gradients for the **hidden layer parameters** $ w_1 $ and $ b_1 $:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{dw_1} = \\frac{1}{N} \\sum \\frac{dL}{dz_1} \\cdot x\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dL}{db_1} = \\frac{1}{N} \\sum \\frac{dL}{dz_1}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Update Weights & Biases Using Gradient Descent**\n",
        "Once we have the gradients, we update the weights and biases using **gradient descent**:\n",
        "\n",
        "$$\n",
        "w_1 = w_1 - \\eta \\cdot \\frac{dL}{dw_1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_1 = b_1 - \\eta \\cdot \\frac{dL}{db_1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_2 = w_2 - \\eta \\cdot \\frac{dL}{dw_2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_2 = b_2 - \\eta \\cdot \\frac{dL}{db_2}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ \\eta $ (learning rate) determines the **step size** for the update.\n",
        "- A **larger** $ \\eta $ makes learning **faster** but can cause instability.\n",
        "- A **smaller** $ \\eta $ makes learning **slower** but more stable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "### **What Happens in Backpropagation?**\n",
        "1. **Compute loss derivative w.r.t output activation** $ a_2 $.\n",
        "2. **Propagate gradient backward to compute gradients** for $ w_2 $ and $ b_2 $.\n",
        "3. **Propagate further back** to compute gradients for $ w_1 $ and $ b_1 $ using Leaky ReLU.\n",
        "4. **Update all parameters** using **gradient descent**.\n",
        "\n",
        "This ensures that weights and biases move in the **direction that minimizes the loss**, improving the network’s accuracy over time. 🚀\n",
        "\n"
      ],
      "metadata": {
        "id": "1YmYB0UIs_0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Training Loop**\n",
        "- Runs forward and backward pass for `epochs` iterations.\n",
        "- Prints loss every 5 epochs.\n"
      ],
      "metadata": {
        "id": "qA2cWd0StIFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        \"\"\"Initializes the neural network.\n",
        "\n",
        "        Args:\n",
        "            layer_sizes: A list specifying the number of neurons in each layer.\n",
        "                         Example: [1, 4, 3, 1] represents a network with:\n",
        "                             - 1 input neuron\n",
        "                             - 4 neurons in the first hidden layer\n",
        "                             - 3 neurons in the second hidden layer\n",
        "                             - 1 output neuron\n",
        "        \"\"\"\n",
        "        self.num_layers = len(layer_sizes) - 1  # Number of weight matrices (layers - 1)\n",
        "        self.weights = []  # List to store weight matrices\n",
        "        self.biases = []  # List to store bias vectors\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # Initialize weights with small random values to prevent exploding gradients\n",
        "            self.weights.append(np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.1)\n",
        "            # Initialize biases to zero\n",
        "            self.biases.append(np.zeros((layer_sizes[i+1], 1)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"Performs the forward propagation step.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (features x samples).\n",
        "\n",
        "        Returns:\n",
        "            activations: A list of activations for each layer.\n",
        "            zs: A list of pre-activation values (Z = W*A + b) for each layer.\n",
        "        \"\"\"\n",
        "        activations = [X]  # Input is the first activation\n",
        "        zs = []  # Store pre-activation values\n",
        "\n",
        "        for i in range(self.num_layers - 1):  # Loop through hidden layers\n",
        "            Z = np.dot(self.weights[i], activations[-1]) + self.biases[i]  # Calculate pre-activation\n",
        "            A = leaky_relu(Z)  # Apply Leaky ReLU activation\n",
        "            zs.append(Z)\n",
        "            activations.append(A)\n",
        "\n",
        "        # Output layer (linear activation)\n",
        "        Z_out = np.dot(self.weights[-1], activations[-1]) + self.biases[-1]\n",
        "        zs.append(Z_out)\n",
        "        activations.append(Z_out)\n",
        "\n",
        "        return activations, zs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_loss(self, predictions):\n",
        "        \"\"\"Calculates the Mean Squared Error (MSE) loss.\n",
        "\n",
        "        Args:\n",
        "            predictions: The model's predictions.\n",
        "\n",
        "        Returns:\n",
        "            The MSE loss.\n",
        "        \"\"\"\n",
        "        return np.mean(0.5 * (y_train - predictions) ** 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backward_pass(self, activations, zs):\n",
        "        \"\"\"Performs the backpropagation step to update weights and biases.\n",
        "\n",
        "        Args:\n",
        "            activations: A list of activations for each layer.\n",
        "            zs: A list of pre-activation values for each layer.\n",
        "        \"\"\"\n",
        "        dW = [None] * self.num_layers  # List to store weight gradients\n",
        "        dB = [None] * self.num_layers  # List to store bias gradients\n",
        "\n",
        "        # Gradient of the output (MSE derivative)\n",
        "        dA = activations[-1] - y_train\n",
        "\n",
        "        for i in reversed(range(self.num_layers)):  # Loop backwards through layers\n",
        "            current_z = zs[i]  # Current layer's pre-activation\n",
        "\n",
        "            if i == self.num_layers - 1:\n",
        "                # Output layer has linear activation, derivative is 1\n",
        "                dZ = dA\n",
        "            else:\n",
        "                # Hidden layers use Leaky ReLU derivative\n",
        "                dZ = dA * leaky_relu_derivative(current_z)\n",
        "\n",
        "            # Compute gradients\n",
        "            dW[i] = np.dot(dZ, activations[i].T) / NUM_SAMPLES\n",
        "            dB[i] = np.mean(dZ, axis=1, keepdims=True)\n",
        "\n",
        "            # Propagate gradient to previous layer\n",
        "            if i > 0:\n",
        "                dA = np.dot(self.weights[i].T, dZ)\n",
        "\n",
        "        # Update parameters (weights and biases) using gradient descent\n",
        "        for i in range(self.num_layers):\n",
        "            self.weights[i] -= LEARNING_RATE * dW[i]\n",
        "            self.biases[i] -= LEARNING_RATE * dB[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, epochs=50):\n",
        "        \"\"\"Trains the neural network for a specified number of epochs.\n",
        "\n",
        "        Args:\n",
        "            epochs: The number of training iterations.\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            activations, zs = self.forward_pass(x_train)  # Forward pass\n",
        "            loss = self.compute_loss(activations[-1])  # Calculate loss\n",
        "            self.backward_pass(activations, zs)  # Backward pass (update parameters)\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                print(f\"Epoch {epoch} - Loss: {loss:.6f}\")  # Print loss every 5 epochs\n"
      ],
      "metadata": {
        "id": "fSCc23svtrpm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. Train the Model**\n",
        "- Initializes the network with `[1, 1, 1]` layers.\n",
        "- Trains for `50` epochs.\n"
      ],
      "metadata": {
        "id": "DaZxFANKtOEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network structure and train the model\n",
        "layer_sizes = [1, 1, 1]  # 1 input, 1 hidden, 1 output\n",
        "nn = NeuralNetwork(layer_sizes)\n",
        "nn.train(epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoXNqzKqtKB6",
        "outputId": "97af746b-a35c-4546-9d31-96ee99b43ea9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 5.833616\n",
            "Epoch 5 - Loss: 2.902166\n",
            "Epoch 10 - Loss: 1.879783\n",
            "Epoch 15 - Loss: 1.522089\n",
            "Epoch 20 - Loss: 1.394850\n",
            "Epoch 25 - Loss: 1.344758\n",
            "Epoch 30 - Loss: 1.313780\n",
            "Epoch 35 - Loss: 1.271630\n",
            "Epoch 40 - Loss: 1.189018\n",
            "Epoch 45 - Loss: 1.035178\n",
            "Epoch 50 - Loss: 0.815966\n",
            "Epoch 55 - Loss: 0.610937\n",
            "Epoch 60 - Loss: 0.482205\n",
            "Epoch 65 - Loss: 0.412919\n",
            "Epoch 70 - Loss: 0.375028\n",
            "Epoch 75 - Loss: 0.353981\n",
            "Epoch 80 - Loss: 0.342316\n",
            "Epoch 85 - Loss: 0.335883\n",
            "Epoch 90 - Loss: 0.332349\n",
            "Epoch 95 - Loss: 0.330413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network structure and train the model\n",
        "layer_sizes = [1, 4, 3, 1]  # 1 input, 1 hidden, 1 output\n",
        "nn = NeuralNetwork(layer_sizes)\n",
        "nn.train(epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jZld6SOtVSl",
        "outputId": "239b8910-b829-4924-cb9c-91d1b828d142"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 5.823733\n",
            "Epoch 5 - Loss: 2.717339\n",
            "Epoch 10 - Loss: 0.823156\n",
            "Epoch 15 - Loss: 0.320812\n",
            "Epoch 20 - Loss: 0.202949\n",
            "Epoch 25 - Loss: 0.134669\n",
            "Epoch 30 - Loss: 0.105323\n",
            "Epoch 35 - Loss: 0.088645\n",
            "Epoch 40 - Loss: 0.076021\n",
            "Epoch 45 - Loss: 0.066310\n",
            "Epoch 50 - Loss: 0.058694\n",
            "Epoch 55 - Loss: 0.052586\n",
            "Epoch 60 - Loss: 0.047570\n",
            "Epoch 65 - Loss: 0.043348\n",
            "Epoch 70 - Loss: 0.039710\n",
            "Epoch 75 - Loss: 0.036507\n",
            "Epoch 80 - Loss: 0.033634\n",
            "Epoch 85 - Loss: 0.031017\n",
            "Epoch 90 - Loss: 0.028605\n",
            "Epoch 95 - Loss: 0.026363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYJ4h593wguM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}